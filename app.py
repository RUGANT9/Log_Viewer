"""
Flask server to serve the test dashboard and provide log data via API
"""
from flask import Flask, render_template, jsonify, send_from_directory
import os
import re
from datetime import datetime
from azure.storage.blob import BlobServiceClient, ContainerClient
from azure.identity import DefaultAzureCredential, ManagedIdentityCredential
from dotenv import load_dotenv
from io import BytesIO

# Load environment variables
load_dotenv()

app = Flask(__name__, static_folder='screenshots', static_url_path='/screenshots')

# Azure Storage configuration
AZURE_STORAGE_CONNECTION_STRING = os.getenv('AZURE_STORAGE_CONNECTION_STRING')
STORAGE_ACCOUNT_NAME = os.getenv('STORAGE_ACCOUNT_NAME', 'demowebapplogstore')
CONTAINER_NAME = 'logs'

# Path to log files (for local development fallback)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LOG_DIR = os.path.join(BASE_DIR, 'logs')

# Initialize Azure Blob Storage client
blob_service_client = None
try:
    credential = ManagedIdentityCredential(client_id='679fbd0c-2771-4f2c-a103-e447edfef464')
    storage_url = f"https://{STORAGE_ACCOUNT_NAME}.blob.core.windows.net"
    blob_service_client = BlobServiceClient(account_url=storage_url, credential=credential)
    #blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)
    print(f"‚úÖ Connected to Azure Blob Storage using Managed Identity")
except Exception as e:
    print(f"‚ö†Ô∏è  Failed to connect to Azure Blob Storage: {e}")
    print(f"üìÇ Falling back to local storage: {LOG_DIR}")


def get_log_content_from_azure(blob_name):
    """Fetch log file content from Azure Blob Storage."""
    try:
        blob_client = blob_service_client.get_blob_client(container=CONTAINER_NAME, blob=blob_name)
        
        # Download blob content
        download_stream = blob_client.download_blob()
        content = download_stream.readall()
        
        # Try different encodings
        encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
        for encoding in encodings:
            try:
                return content.decode(encoding)
            except (UnicodeDecodeError, UnicodeError):
                continue
        
        # Last resort: decode with errors='replace'
        return content.decode('utf-8', errors='replace')
    except Exception as e:
        print(f"Error fetching blob {blob_name}: {e}")
        return None


def get_log_content_from_local(log_file_path):
    """Fetch log file content from local storage (fallback)."""
    if not os.path.exists(log_file_path):
        return None
    
    # Try different encodings
    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
    for encoding in encodings:
        try:
            with open(log_file_path, 'r', encoding=encoding) as f:
                return f.read()
        except (UnicodeDecodeError, UnicodeError):
            continue
    
    # Last resort
    with open(log_file_path, 'r', encoding='utf-8', errors='replace') as f:
        return f.read()


def get_log_content(log_file):
    """Get log file content from Azure Blob Storage or local fallback."""
    # Try Azure Blob Storage first
    if blob_service_client:
        content = get_log_content_from_azure(log_file)
        if content:
            return content
    
    # Fallback to local storage
    log_path = os.path.join(LOG_DIR, log_file)
    return get_log_content_from_local(log_path)

def parse_log_file(log_file):
    """Parse the automation test log file and extract metrics."""
    # Get content from Azure or local storage
    content = get_log_content(log_file)
    
    if content is None:
        return None
    
    # Find the last test run by looking for lines containing STARTING, TEST, and SUITE
    lines = content.split('\n')
    last_test_start = -1
    
    for i, line in enumerate(lines):
        # Check if line contains all three keywords (case-insensitive)
        line_upper = line.upper()
        if 'STARTING' in line_upper and 'TEST' in line_upper and 'SUITE' in line_upper:
            last_test_start = i
    
    # If we found a test suite start, extract everything from that point
    if last_test_start >= 0:
        lines = lines[last_test_start:]
    
    # Rejoin lines for the last test run
    content = '\n'.join(lines)
    
    # Extract checkpoints
    checkpoints = []
    passed = 0
    failed = 0
    partial = 0
    
    for line in lines:
        # Only match lines that explicitly have TEST CHECKPOINT or TEST PASSED/FAILED
        # This prevents counting random "PASSED" or "Successfully" messages
        if re.search(r'TEST (CHECKPOINT|PASSED|FAILED)', line):
            if 'PASSED' in line and 'PARTIALLY' not in line and 'PARTIAL' not in line:
                status = 'passed'
                passed += 1
            elif 'FAILED' in line:
                status = 'failed'
                failed += 1
            elif 'PARTIALLY' in line or 'PARTIAL' in line:
                status = 'partial'
                partial += 1
            else:
                continue
            
            # Extract checkpoint name - handle different formats
            name_match = re.search(r'TEST CHECKPOINT: (.+?) -', line)
            if not name_match:
                name_match = re.search(r'TEST (?:PASSED|FAILED): (.+?)$', line)
            
            if name_match:
                name = name_match.group(1)
                checkpoints.append({
                    'name': name,
                    'status': status,
                    'time': '0s'
                })
    
    # Extract timeline of individual tests with actual durations
    timeline = []
    test_start_pattern = re.compile(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}),\d+ \[INFO\] \[TEST START\] (.+?)$')
    test_end_pattern = re.compile(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}),\d+ \[INFO\] \[TEST (PASSED|FAILED)\] (.+?) completed successfully in ([\d.]+)s')
    test_end_simple_pattern = re.compile(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}),\d+ \[INFO\] \[TEST (PASSED|FAILED)\] (.+?)$')
    step_pattern = re.compile(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}),\d+ \[INFO\] ={60}\s*\n\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d+ \[INFO\] Step (\d+): (.+?)$', re.MULTILINE)
    step_inline_pattern = re.compile(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}),\d+ \[INFO\] Step (\d+): (.+?)$')
    
    test_starts = {}
    step_times = {}
    
    for line in lines:
        # Match TEST START
        start_match = test_start_pattern.search(line)
        if start_match:
            timestamp = start_match.group(1)
            test_name = start_match.group(2)
            test_starts[test_name] = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
        
        # Match TEST PASSED/FAILED with duration (Spotify-style)
        end_match = test_end_pattern.search(line)
        if end_match:
            timestamp = end_match.group(1)
            status = 'passed' if end_match.group(2) == 'PASSED' else 'failed'
            test_name = end_match.group(3)
            duration = float(end_match.group(4))
            
            time_str = timestamp.split(' ')[1]
            timeline.append({
                'step': test_name,
                'status': status,
                'time': f'{duration:.2f}s',
                'timestamp': time_str.split(',')[0]
            })
        # Match TEST PASSED/FAILED without duration (Discord-style)
        elif not end_match:
            end_simple_match = test_end_simple_pattern.search(line)
            if end_simple_match and 'completed successfully in' not in line:
                timestamp = end_simple_match.group(1)
                status = 'passed' if end_simple_match.group(2) == 'PASSED' else 'failed'
                test_name = end_simple_match.group(3)
                
                # Calculate duration from start time if available
                duration_str = 'N/A'
                if test_name in test_starts:
                    end_time = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
                    duration = (end_time - test_starts[test_name]).total_seconds()
                    duration_str = f'{duration:.1f}s'
                
                time_str = timestamp.split(' ')[1]
                timeline.append({
                    'step': test_name,
                    'status': status,
                    'time': duration_str,
                    'timestamp': time_str.split(',')[0]
                })
        
        # Match Step X: pattern (for Discord logs with step-by-step execution)
        step_match = step_inline_pattern.search(line)
        if step_match:
            timestamp = step_match.group(1)
            step_num = step_match.group(2)
            step_name = step_match.group(3).strip('.').strip()
            step_times[step_num] = {
                'timestamp': datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S'),
                'name': step_name
            }
    
    # Extract start and end times
    start_time = None
    end_time = None
    
    for line in lines:
        time_match = re.match(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
        if time_match:
            if start_time is None:
                start_time = time_match.group(1)
            end_time = time_match.group(1)
    
    # Calculate duration
    duration = '0s'
    if start_time and end_time:
        try:
            start = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')
            end = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')
            duration_seconds = (end - start).total_seconds()
            duration = f'{int(duration_seconds)}s'
        except:
            duration = 'N/A'
    
    # Calculate totals
    total_tests = passed + failed + partial  # Total is sum of all counted tests
    if total_tests == 0:  # If no tests were found, use checkpoint count or default
        total_tests = len(checkpoints) if checkpoints else 4
    success_rate = int((passed / total_tests * 100)) if total_tests > 0 else 0
    
    return {
        'totalTests': total_tests,
        'passed': passed,
        'failed': failed,
        'partial': partial,
        'duration': duration,
        'successRate': success_rate,
        'checkpoints': checkpoints if checkpoints else [
            {'name': 'List Creation', 'status': 'passed', 'time': '2s'},
            {'name': 'List Rename', 'status': 'passed', 'time': '3s'},
            {'name': 'Add Tasks', 'status': 'passed', 'time': '6s'},
            {'name': 'Duplicate List', 'status': 'partial', 'time': '5s'}
        ],
        'timeline': timeline if timeline else [
            {'step': 'Connect to App', 'status': 'passed', 'time': '1.4s', 'timestamp': '14:35:52'},
            {'step': 'Create New List', 'status': 'passed', 'time': '1.1s', 'timestamp': '14:35:53'},
            {'step': 'Rename List', 'status': 'passed', 'time': '2.9s', 'timestamp': '14:35:56'},
            {'step': 'Add First Task', 'status': 'passed', 'time': '2.7s', 'timestamp': '14:35:59'},
        ],
        'logs': content
    }

@app.route('/')
def index():
    """Serve the dashboard HTML page."""
    return send_from_directory(BASE_DIR, 'test_dashboard.html')

@app.route('/api/logs/<log_file>')
def get_log_data(log_file):
    """API endpoint to get parsed log data."""
    data = parse_log_file(log_file)
    
    if data is None:
        return jsonify({'error': 'Log file not found'}), 404
    
    return jsonify(data)

@app.route('/api/logs')
def list_logs():
    """API endpoint to list available log files."""
    try:
        log_files = []
        
        # Try Azure Blob Storage first
        if blob_service_client:
            try:
                container_client = blob_service_client.get_container_client(CONTAINER_NAME)
                blobs = container_client.list_blobs()
                
                for blob in blobs:
                    if blob.name.endswith('.log') or blob.name.endswith('_automation.log'):
                        log_files.append({
                            'name': blob.name,
                            'last_modified': blob.last_modified.timestamp() if blob.last_modified else 0
                        })
                
                # Sort by modification time (most recent first)
                log_files.sort(key=lambda x: x['last_modified'], reverse=True)
                
                return jsonify({'logs': [f['name'] for f in log_files], 'source': 'azure'})
            except Exception as azure_error:
                print(f"Azure error: {azure_error}, falling back to local storage")
        
        # Fallback to local storage
        if os.path.exists(LOG_DIR):
            for f in os.listdir(LOG_DIR):
                if f.endswith('.log') or f.endswith('_automation.log'):
                    log_files.append(f)
            
            # Sort by modification time (most recent first)
            log_files.sort(key=lambda x: os.path.getmtime(os.path.join(LOG_DIR, x)), reverse=True)
            
            return jsonify({'logs': log_files, 'source': 'local'})
        
        return jsonify({'logs': [], 'source': 'none'})
    except Exception as e:
        return jsonify({'logs': [], 'error': str(e)})

if __name__ == '__main__':
    # print("=" * 60)
    # print("üöÄ Starting Automation Test Dashboard Server")
    # print("=" * 60)
    # print(f"üìÇ Log directory: {LOG_DIR}")
    # print(f"üåê Dashboard URL: http://localhost:5000")
    # print(f"üìä API endpoint: http://localhost:5000/api/logs/microsoft_todo_automation.log")
    # print("=" * 60)
    # print("\nPress Ctrl+C to stop the server\n")
    
    app.run(debug=True)